{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "'''For debugging puopose ,set_trace for setting breakpoint\n",
    "n - > New Line\n",
    "c - > Continue'''\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re #Regex\n",
    "import nltk #Natural language toolkit\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import contractions\n",
    "#from contractions import CONTRACTION_MAP\n",
    "import unicodedata\n",
    "\n",
    "pd.options.display.max_colwidth=200\n",
    "%matplotlib inline\n",
    "#autoreload all modules automatically before entering execution code typed\n",
    "%load_ext autoreload\n",
    "#Reload all modules (Except those excluded by %aimport)\n",
    "%autoreload 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Lemma POS Tag Dep Shape is alpha is stop\n",
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n",
      "\n",
      " pronoun\n"
     ]
    }
   ],
   "source": [
    "#nlp =spacy.load('en_core_web_sm',parse=False,tag=False,entity=False)\n",
    "'''It returns Language object containing all components and data needed to process text'''\n",
    "nlp=spacy.load('en',parse=False,tag=False,entity=False)\n",
    "#print(nlp)\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "print('Text Lemma POS Tag Dep Shape is alpha is stop')\n",
    "for token in doc:#Tokenization\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)\n",
    "#Spacy.expain will show a short descripton\n",
    "#print('\\n',spacy.explain(\"PRON\"))\n",
    "\n",
    "tokenizer=ToktokTokenizer()\n",
    "stopwords_list=nltk.corpus.stopwords.words('english')\n",
    "stopwords_list.remove('no')\n",
    "stopwords_list.remove('not')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPACY\n",
    "* Text: The original word text.\n",
    "2. Lemma: The base form of the word.\n",
    "3. POS: The simple part-of-speech tag.\n",
    "4. Tag: The detailed part-of-speech tag.\n",
    "5. Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "6. Shape: The word shape – capitalization, punctuation, digits.\n",
    "7. is alpha: Is the token an alpha character?\n",
    "8. is stop: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Texts- Strip html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Running the document through Beautiful Soup gives us a BeautifulSoup object,\n",
    "    which represents the document as a nested data structure'''\n",
    "def strip_html_tags(text):\n",
    "    soup=BeautifulSoup(text,\"html.parser\")\n",
    "    #print(soup.prettify())\n",
    "    stripped_text=soup.get_text()#extracting all the text from object\n",
    "    return stripped_text    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing accented characters(Text Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(unicodedata.normalize('NFKD','Amélie').encode('ascii','ignore').decode('utf-8','ignore'))\n",
    "'''This is the process of text normalization'''\n",
    "def remove_accented_chars(text):\n",
    "    text=unicodedata.normalize('NFKD',text).encode('ascii',\n",
    "                                                   'ignore').decode('utf-8','ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expanding Contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "#print(expand_contractions(\"Cant've\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    text = re.sub('[^a-zA-z0-9\\s]',' ',str(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text=nlp(text)\n",
    "    '''If word.lemma_ is not -PRON- then join it\\'s lemma otherwise plain text '''\n",
    "    text=' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text,is_lower_case=False):\n",
    "    tokens=tokenizer.tokenize(text)\n",
    "    tokens=[token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens=[token for token in tokens if token not in stopwords_list]\n",
    "    else:\n",
    "        filtered_tokens=[token for token in tokens if token.lower() not in stopwords_list]\n",
    "        \n",
    "    filtered_text=' '.join(filtered_tokens)\n",
    "        \n",
    "    return filtered_text   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize text corpus - tying it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    set_trace() #Setting breakpoint\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # insert spaces between special characters to isolate them    \n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters    \n",
    "        if special_char_removal:\n",
    "            doc = remove_special_characters(doc)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        #set_trace()\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \\n              It's an amazing language which can be used for Scripting, Web development,\\r\\n\\r\\n\\n              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\\n              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\\n              got a headstart!</p>\\n           \""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = \"\"\"<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \n",
    "              It's an amazing language which can be used for Scripting, Web development,\\r\\n\\r\\n\n",
    "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\n",
    "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\n",
    "              got a headstart!</p>\n",
    "           \"\"\"\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-24-067515717a2e>\u001b[0m(9)\u001b[0;36mnormalize_corpus\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m      7 \u001b[1;33m    \u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      8 \u001b[1;33m    \u001b[1;31m# normalize each document in the corpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m----> 9 \u001b[1;33m    \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     10 \u001b[1;33m        \u001b[1;31m# strip HTML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     11 \u001b[1;33m        \u001b[1;32mif\u001b[0m \u001b[0mhtml_stripping\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello Hello can you hear me I just heard about Python it is an amazing language which can be used for Scripting Web development Information Retrieval Natural Language Processing Machine Learning Artificial Intelligence What are you waiting for Go and get started he is learning she is learning they have already got a headstart ']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_corpus([document],text_lemmatization=False,\n",
    "                 stopword_removal=False,text_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-24-067515717a2e>\u001b[0m(9)\u001b[0;36mnormalize_corpus\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m      7 \u001b[1;33m    \u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      8 \u001b[1;33m    \u001b[1;31m# normalize each document in the corpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m----> 9 \u001b[1;33m    \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     10 \u001b[1;33m        \u001b[1;31m# strip HTML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     11 \u001b[1;33m        \u001b[1;32mif\u001b[0m \u001b[0mhtml_stripping\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hello hello hear hear python amazing language use script web development information retrieval natural language processing machine learning artificial intelligence wait go get start learn learn already get headstart']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_corpus([document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
